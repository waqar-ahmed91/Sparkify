{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sparkify.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Overview\n",
    "This project is all about analyzing and predicting the churn rate of the users from the data provided by a fictional music streaming company called 'Sparkify'. Users used this platform as a free member and paid member. While using the service data is generated in terms of playing a song, visiting a page etc.. According to the company some users downgraded their services from paid to free and some leave the services after using it for some time. So the main purpose of this project is to design a model that can predict the churn rate of the users according to the data provided.\n",
    "\n",
    "# Problem Statement\n",
    "Predict the churn rate of the users i.e if a user downgrade his/her services to free from paid subscription or leave the services/deactivate his/her account from the platform. For this purpose a machine learning model will be build from using different features to predict the churn of a new customer according to his/her activity while using the Sparkify platform.\n",
    "\n",
    "The full dataset is about 12 GB in space which is suitable for Amazon EMR cluster while we use the mini dataset that is 128 MB in size to carry on the task and prepare all the necessary tasks before going for the full dataset. Apache Spark engine will be used to process and analyze the large scale data and as it is a classfication problem to predict if users churn or not so following machine learning techniques will be used to predict the churn rate of the users.\n",
    "\n",
    "1. Linear Regression\n",
    "2. Random Forest Classifier\n",
    "\n",
    "Other than these above mentioned models there are other classification models which can be used to check and get even better performance.\n",
    "\n",
    "# Metrics\n",
    "f1-score (combination of precision and recall) will be used to measure and evaluate the performance of the machine learning model on the dataset provided. The main reason to use f1-score to measure the performance is because the churned user data is imbalance in the dataset due to its unequal distribution so to achieve a balanced results f1-score will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "After importing the required libraries premilinary data wrangling was performed on the dataset to check and clean the data for exploration purpose.\n",
    "\n",
    "The dataset was messy and most of the datatypes are incorrect as well as some of the data are missing so thorough data wrangling process is used to clean the data as much as possible to make the data consistent and suitable for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Size\n",
    "There are **286500** rows and **18** columns are in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the data Analysis there are:\n",
    "\n",
    "121 <b>male</b> and\n",
    "\n",
    "104 <b>female</b> users are in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](gender_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paid and Free Users\n",
    "![title](pf_users.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 195 free and 165 paid users on the Sparkify App/Platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churned and Non-Churned Users\n",
    "![title](churn_users.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of churned users are 52 while 173 users are still using the App."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churn Rate Among Genders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](gender.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to above data more male members churned from the platform as compared to female members while the number of male members are higher as compared to females"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Churned Rate as Per the Level of Using Service\n",
    "36 users churned from the paid services while most of the users are using the free services of the platform\n",
    "\n",
    "![title](level_churn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Users Location\n",
    "Majority of the users are from Los Angeles while users from New York left or downgraded the services from paid to free.\n",
    "![title](location.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "After some thorough data analysis and visualization few of the features are taken from the dataset to train the model to predict the new users churn rate. For this purpose following features has been selected after thorough cleaning and observations\n",
    "![title](feature_selection.png)\n",
    "\n",
    "During the feature engineering process to make the data ready for Apache Spark further preprocessing required to make the data according to PySpark standards to fit and train the model with or without parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the models churned column name from the above dataset is changed to **label**.\n",
    "\n",
    "![title](label_name.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feature engineering and selection data is split into train and test set as well as vector assember and standard scaler is used on the dataset for training the models and getting the better f1-score.\n",
    "\n",
    "![title](modeling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "**Logistic Regression** and **Random Forest Classifier** has been used to fit the train data to predict the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "![title](lr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression f1-score\n",
    "\n",
    "![title](lr_f1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1-score for Logistic Regression was approximately **65.5%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "![title](rf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest f1-score\n",
    "![title](rf_f1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1-score for Random Forest was approximately **65.5%** somehow same as Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refinement\n",
    "\n",
    "After training the data on both models with default parameters then we further checked the models by tuning the hyper parameters for this purpose we defined parameter grids for both Logistic Regression and Random Forest Classifier then fit the models on those parameters grid with the help of cross validation to increase the f1-score while doing so took a bit of time to train the models than default parameters while the result of f1-score came out somehow similar to the previous default parameters models.\n",
    "\n",
    "So extra features as well as other classification machine learning models need to be checked to compare the f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Validation\n",
    "\n",
    "As such after tuning the hyperparameters for both models the result was somehow same for both models and both models took similar amount of time to fit and train the model even after using the 2 fold cross validation on the dataset the results were pretty much similar.\n",
    "\n",
    "so further fine tuning and different models need to be cross validated to get the better results in terms of f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justification\n",
    "f1-score for both of the models somehow above 60 percent i.e ~65.5% but in terms of real world application further enhancement of the model and tuning of the models as well as checking the other models on the dataset is necessary to achieve best f1-score results to make the model real world ready for the prediction of users churn. For this purpose naive bayes, SVC, Gradient Boost Classifier, XGBoost etc.. will be used in future to better the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "According to the above f1-scores both the models Logistic Regression and Random Forest performed same in terms of precision and recall having f1-score approximately **65.5%**.\n",
    "\n",
    "As the data was messy so it took a bit of time to preprocess the data for exploration and feature engineering even after preprocessing the data during the feature engineering process some more preprocessing was required to make the dataset ready for the Apache Spark modeling purpose.\n",
    "\n",
    "The models has been fit with and without paramter tuning but the results are somehow same while adding and tuning the parameters just increase the running time of fitting the model.\n",
    "\n",
    "As the dataset is too small as compared to the full dataset of 12 GB so it is better to move to the Amazon EMR Cluster to perform these above steps through EDA to featuring engineering then modeling and fine tuning of the model to get even better results in terms of churn rate prediction.\n",
    "\n",
    "### Further Improvements\n",
    "1. Different models can be used to train the data other than Logistic and Random Forest to compare the f1-score of those models with the above ones.\n",
    "2. More parameter grids can be used to train the model which definitely take a lot of time.\n",
    "3. Large number of dataset can be used to train the models to predict the churn rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
